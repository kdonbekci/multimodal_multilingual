{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d08ddb-8799-45c8-a271-b3399c6ef4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94ef356-ffb7-4f8d-9c32-872a214f0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_visual_embeddings = torch.rand(\n",
    "    64, 300, 256\n",
    ")  # batch_size, timesteps, visual_embedding_dim\n",
    "dummy_audio_embeddings = torch.rand(\n",
    "    64, 600, 1024\n",
    ")  # batch_size, timesteps, audio_embedding_dim\n",
    "dummy_text_embeddings = torch.rand(\n",
    "    64, 50, 768\n",
    ")  # batch_size, timesteps, audio_embedding_dim\n",
    "\n",
    "# with text, the 1st timestep is the CLS token, RoBERTa already adds the CLS token at the\n",
    "# beginning of the sequence for each element in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f43320-7d6f-4390-a9b5-4b344392d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = dummy_audio_embeddings\n",
    "text_embeddings_cls = dummy_text_embeddings\n",
    "visual_embeddings = dummy_visual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2379d76a-fcd1-4676-b74a-6df924791fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_visual = torch.randn(768, requires_grad=True)\n",
    "cls_audio = torch.randn(768, requires_grad=True)\n",
    "visual_projection = nn.Linear(in_features=256, out_features=768)\n",
    "audio_projection = nn.Linear(in_features=1024, out_features=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6940a140-f3e8-49b8-a43e-ca9d6472cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings_cls = torch.cat(\n",
    "    (\n",
    "        cls_audio.expand(audio_embeddings.shape[0], 1, cls_audio.shape[0]),\n",
    "        audio_projection(audio_embeddings),\n",
    "    ),\n",
    "    dim=1,\n",
    ")\n",
    "visual_embeddings_cls = torch.cat(\n",
    "    (\n",
    "        cls_visual.expand(visual_embeddings.shape[0], 1, cls_visual.shape[0]),\n",
    "        visual_projection(visual_embeddings),\n",
    "    ),\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "audio_embeddings_cls = audio_embeddings_cls.permute(1, 0, 2)\n",
    "x_text = text_embeddings_cls.permute(1, 0, 2)\n",
    "visual_embeddings_cls = visual_embeddings_cls.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fef4ff-cd88-4aec-837d-d60ca94a102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[timesteps, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68db8384-c217-4540-9d6f-e402f0e4a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the paper (https://ieeexplore.ieee.org/document/9206016), they encode these pretrained embeddings with\n",
    "# sinusodial positional encodings, which is a bit strange. Wav2Vec2 achieves sort of a positional \"encoding\"\n",
    "# via its convolutional layers\n",
    "positional_encoder_visual = PositionalEncoding(embedding_dim=768, max_len=400)\n",
    "positional_encoder_audio = PositionalEncoding(embedding_dim=768, max_len=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0f5f61-209e-437d-9ecb-74de23504282",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeddings_cls = positional_encoder_visual(visual_embeddings_cls)\n",
    "audio_embeddings_cls = positional_encoder_audio(audio_embeddings_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236a3a63-2889-4188-97e7-0c5644826566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Self-Attention layers have residual connections and are followed by Layer Normalization as described\n",
    "# in the paper \"Attention is All You Need\"\n",
    "visual_self_attention = nn.MultiheadAttention(\n",
    "    embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768\n",
    ")\n",
    "audio_self_attention = nn.MultiheadAttention(\n",
    "    embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d4f133b7-ef71-413a-9efd-e63ee960f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_visual, _ = visual_self_attention(\n",
    "    query=visual_embeddings_cls,\n",
    "    value=visual_embeddings_cls,\n",
    "    key=visual_embeddings_cls,\n",
    "    need_weights=False,\n",
    "    attn_mask=None\n",
    ")\n",
    "x_audio, _ = audio_self_attention(\n",
    "    query=audio_embeddings_cls,\n",
    "    value=audio_embeddings_cls,\n",
    "    key=audio_embeddings_cls,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0bd6a641-067a-4b4f-8ed0-c79a1e2f6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_visual = x_visual + visual_embeddings_cls\n",
    "x_audio = x_audio + audio_embeddings_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e13a7c0c-9418-4087-a34c-7288cf58b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_layernorm = nn.LayerNorm(normalized_shape=768)\n",
    "audio_layernorm = nn.LayerNorm(normalized_shape=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c65c8b3e-b35b-45ec-a9c4-10a26ab0593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_visual = visual_layernorm(x_visual)\n",
    "x_audio = audio_layernorm(x_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c37ebcac-5ec9-4aea-894c-92257fba96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IMA layers are between a host modality's CLS token and the entire embedding sequence of a target\n",
    "# modality. So there are a total of 6 IMA Attention blocks, 1 for each possible pairwise permutation of \n",
    "# the three modalities. \n",
    "\n",
    "audio_visual_ima = nn.MultiheadAttention(embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768)\n",
    "audio_visual_layernorm = nn.LayerNorm(normalized_shape=768)\n",
    "audio_text_ima = nn.MultiheadAttention(embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768)\n",
    "audio_text_layernorm = nn.LayerNorm(normalized_shape=768)\n",
    "\n",
    "visual_audio_ima = nn.MultiheadAttention(embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768)\n",
    "visual_audio_layernorm = nn.LayerNorm(normalized_shape=768)\n",
    "visual_text_ima = nn.MultiheadAttention(embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768)\n",
    "visual_text_layernorm = nn.LayerNorm(normalized_shape=768)\n",
    "\n",
    "text_visual_ima = nn.MultiheadAttention(embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768)\n",
    "text_visual_layernorm = nn.LayerNorm(normalized_shape=768)\n",
    "text_audio_ima = nn.MultiheadAttention(embed_dim=768, num_heads=4, dropout=0.0, kdim=768, vdim=768)\n",
    "text_audio_layernorm = nn.LayerNorm(normalized_shape=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bddba487-2ea5-4854-9867-9986e195cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_audio_visual_cls, _ = audio_visual_ima(\n",
    "    query=x_audio[0:1], # note the indexing here, using only the 1st token, which is the CLS token as the query vector.\n",
    "    value=x_visual,\n",
    "    key=x_visual,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")\n",
    "x_audio_visual_cls = audio_visual_layernorm(x_audio_visual_cls + x_audio[0:1])\n",
    "x_audio_text_cls, _ = audio_text_ima(\n",
    "    query=x_audio[0:1],\n",
    "    value=x_text,\n",
    "    key=x_text,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")\n",
    "x_audio_text_cls = audio_text_layernorm(x_audio_text_cls + x_audio[0:1])\n",
    "\n",
    "x_visual_audio_cls, _ = visual_audio_ima(\n",
    "    query=x_visual[0:1],\n",
    "    value=x_audio,\n",
    "    key=x_audio,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")\n",
    "x_visual_audio_cls = visual_audio_layernorm(x_visual_audio_cls + x_visual[0:1])\n",
    "x_visual_text_cls, _ = visual_text_ima(\n",
    "    query=x_visual[0:1],\n",
    "    value=x_text,\n",
    "    key=x_text,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")\n",
    "x_visual_text_cls = visual_text_layernorm(x_visual_text_cls + x_visual[0:1])\n",
    "\n",
    "x_text_visual_cls, _ = text_visual_ima(\n",
    "    query=x_text[0:1],\n",
    "    value=x_visual,\n",
    "    key=x_visual,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")\n",
    "x_text_visual_cls = text_visual_layernorm(x_text_visual_cls + x_text[0:1])\n",
    "x_text_audio_cls, _ = text_audio_ima(\n",
    "    query=x_text[0:1],\n",
    "    value=x_audio,\n",
    "    key=x_audio,\n",
    "    need_weights=False,\n",
    "    attn_mask=None,\n",
    ")\n",
    "x_text_audio_cls = text_audio_layernorm(x_text_audio_cls + x_text[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1e17b7a1-c09a-4470-ace9-ac558c886656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two fused intermodal representations are further combined via element-wise multiplication. In the paper of \n",
    "# the project, they saw a performance boost with this apprach compared to concatenation.\n",
    "x_fused_audio = torch.squeeze(x_audio_text_cls * x_audio_visual_cls)\n",
    "x_fused_visual = torch.squeeze(x_visual_audio_cls * x_visual_text_cls)\n",
    "x_fused_text = torch.squeeze(x_text_audio_cls * x_text_visual_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b1b708d0-f77c-4847-aaec-633be0761b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fused_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "121ac594-e494-4bb8-acb4-0ae600758dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fused_multimodal = torch.cat((x_fused_audio, x_fused_visual, x_fused_text), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8b24dcf1-3fb7-4494-a99b-aa6ba1a7f8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2304])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fused_multimodal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "acd60035-4f0b-44f5-95fe-b58f0bd6e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_projection = nn.Linear(in_features=768*3, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "90864d29-4a22-4459-b1da-6a415328ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = output_projection(x_fused_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0b026d4b-4353-4a42-9455-77d24d906c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
