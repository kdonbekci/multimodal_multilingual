{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e924ac90-96c8-4dc9-aa91-84a376753afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F_vision\n",
    "import pickle\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a70d8d-5cb8-4e5d-9be6-1b9946633e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87182864-2e3d-4c1b-848c-4ec34c4f14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(Enum):\n",
    "    TRAIN = 1\n",
    "    VALIDATION = 2\n",
    "    TEST = 3\n",
    "\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    POSITIVE = 1\n",
    "    NEUTRAL = 2\n",
    "    NEGATIVE = 3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metadata:\n",
    "    path: str\n",
    "    video_id: str\n",
    "    clip_id: int\n",
    "    duration: float\n",
    "    # VISUAL INFO\n",
    "    width: int\n",
    "    height: int\n",
    "    # AUDIO INFO\n",
    "    sample_rate: int\n",
    "    channels: int\n",
    "    # TEXT INFO\n",
    "    text: str\n",
    "    sentiment_score: float\n",
    "    sentiment: Sentiment\n",
    "    split: Split\n",
    "\n",
    "    def get_cv2_capture(self) -> cv2.VideoCapture:\n",
    "        return cv2.VideoCapture(self.path)\n",
    "\n",
    "    def play_video(self):\n",
    "        pprint(self)\n",
    "        return ipd.Video(self.path)\n",
    "\n",
    "    def get_video_frames(self, fps=10) -> np.ndarray:\n",
    "        # note that due to the variable nature of FPS in encoded video,\n",
    "        # the fps argument is only approximate for this function\n",
    "        cap = self.get_cv2_capture()\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video file\")\n",
    "\n",
    "        frame_interval = 1.0 / fps  # interval in seconds\n",
    "\n",
    "        frames = []\n",
    "        last_capture_time = -frame_interval\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Get the current time of the frame in the video\n",
    "            current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # convert to seconds\n",
    "\n",
    "            # Check if the current frame is due for capture based on desired FPS\n",
    "            if current_time >= (last_capture_time + frame_interval):\n",
    "                frames.append(frame)\n",
    "                last_capture_time = current_time\n",
    "\n",
    "        cap.release()\n",
    "        frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames]\n",
    "        return np.array(frames)\n",
    "\n",
    "    def get_audio(self) -> torch.Tensor:\n",
    "        audio, sample_rate = torchaudio.load(self.path)\n",
    "        return audio, sample_rate\n",
    "\n",
    "\n",
    "class RawDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        dataset_name: str,\n",
    "        max_workers: int = 8,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.dataset_dir = Path(data_dir) / dataset_name\n",
    "        self.labels = pd.read_csv(self.dataset_dir / \"label.csv\")\n",
    "        if debug:\n",
    "            max_workers = 1\n",
    "            self.labels = self.labels.sample(n=100)\n",
    "        if max_workers > 1:\n",
    "            with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "                splits = np.array_split(self.labels, max_workers)\n",
    "                results = list(executor.map(self.build_video_sublist, splits))\n",
    "            self.merge_lists(results)\n",
    "        else:\n",
    "            self.videos = self.build_video_sublist(self.labels)\n",
    "\n",
    "    def merge_lists(self, results: list[dict, set]):\n",
    "        # used for merging different results from different processors\n",
    "        # if ProcessPoolExecutor is used\n",
    "        self.videos = []\n",
    "        for videos in results:\n",
    "            self.videos += videos\n",
    "\n",
    "    def read_video_metadata(self, video_path: Path, row) -> Metadata:\n",
    "        # VIDEO METADATA\n",
    "        vid = cv2.VideoCapture(filename=str(video_path))\n",
    "        width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # AUDIO METADATA\n",
    "        audio, sample_rate = torchaudio.load(str(video_path), normalize=False)\n",
    "        channels, audio_sample_count = audio.shape\n",
    "        duration = audio_sample_count / sample_rate\n",
    "\n",
    "        # LABELS\n",
    "        if row.annotation == \"Positive\":\n",
    "            sentiment = Sentiment.POSITIVE\n",
    "        elif row.annotation == \"Neutral\":\n",
    "            sentiment = Sentiment.NEUTRAL\n",
    "        elif row.annotation == \"Negative\":\n",
    "            sentiment = Sentiment.NEGATIVE\n",
    "        else:\n",
    "            raise ValueError(f\"{row.annotation=} not recognized\")\n",
    "\n",
    "        # SPLIT\n",
    "        if row.mode == \"train\":\n",
    "            split = Split.TRAIN\n",
    "        elif row.mode == \"valid\":\n",
    "            split = Split.VALIDATION\n",
    "        elif row.mode == \"test\":\n",
    "            split = Split.TEST\n",
    "        else:\n",
    "            raise ValueError(f\"{row.mode=} not recognized\")\n",
    "\n",
    "        return Metadata(\n",
    "            path=str(video_path),\n",
    "            duration=duration,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            sample_rate=sample_rate,\n",
    "            channels=channels,\n",
    "            video_id=row.video_id,\n",
    "            clip_id=row.clip_id,\n",
    "            text=row.text,\n",
    "            sentiment_score=row.label,\n",
    "            sentiment=sentiment,\n",
    "            split=split,\n",
    "        )\n",
    "\n",
    "    def build_video_sublist(self, df: pd.DataFrame):\n",
    "        # single processor code for reading metadata of videos from a Pandas DataFrame\n",
    "        videos = []\n",
    "        for row in df.itertuples():\n",
    "            video_path = self.dataset_dir / \"Raw\" / row.video_id / f\"{row.clip_id}.mp4\"\n",
    "            assert video_path.exists(), f\"{video_path=} does not exist\"\n",
    "            videos.append(self.read_video_metadata(video_path=video_path, row=row))\n",
    "        return videos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.videos[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7c048d9e-e400-4faf-9305-17328afcf122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to re-extract metadata, uncomment and run the lines below, they are however cached as\n",
    "# # pickles and can be loaded instead\n",
    "# ch_sims = RawDataset(data_dir=data_dir, dataset_name=\"CH-SIMS\", debug=False)\n",
    "# with open('./data/CH-SIMS.pkl', 'wb') as f:\n",
    "#     pickle.dump(ch_sims, f)\n",
    "# cmu_mosei = RawDataset(data_dir=data_dir, dataset_name=\"CMU-MOSEI\", debug=False)\n",
    "# with open('./data/CMU-MOSEI.pkl', 'wb') as f:\n",
    "#     pickle.dump(cmu_mosei, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8cd3e61-a6ba-45c4-a2eb-8c6af8ee7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/CMU-MOSEI.pkl\", \"rb\") as f:\n",
    "    cmu_mosei = pickle.load(f)\n",
    "with open(\"./data/CH-SIMS.pkl\", \"rb\") as f:\n",
    "    ch_sims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6314a2a5-3699-4fbe-a6cb-78d0f4bf0ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22856"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmu_mosei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99f50da-4df1-4aa8-a593-b30b25f9ca90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4403"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_sims)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "035eec48-80ac-45cd-866f-c7ea9c8e721c",
   "metadata": {},
   "source": [
    "CMU-MOSEI problem examples\n",
    "3213: the text is positive leaning according to Chat GPT but visual clues hint negative\n",
    "5522: Very Long video that can be chopped up using forced alignment\n",
    "21807: Non human, animation\n",
    "5551: small face, AI upsampling\n",
    "5921: No face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "deb4b642-c146-4f0d-bca3-2c2aef0b0c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean duration: 6.964760340764422\n",
      "90th percentile: 12.634512471655329\n",
      "95th percentile: 15.91903401360543\n",
      "99th percentile: 23.6076707482993\n"
     ]
    }
   ],
   "source": [
    "durations = np.array([x.duration for x in cmu_mosei] + [x.duration for x in ch_sims])\n",
    "print(f\"mean duration: {np.mean(durations)}\")\n",
    "print(f\"90th percentile: {np.percentile(durations, 90)}\")\n",
    "print(f\"95th percentile: {np.percentile(durations, 95)}\")\n",
    "print(f\"99th percentile: {np.percentile(durations, 99)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b7e3a2-567f-4a72-818b-da7177a83278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "# we are using Wav2Vec2-XLS-R as it is trained on multilingual data\n",
    "audio_model = Wav2Vec2Model.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", cache_dir=\"models/\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c94def-2c20-4f61-b5e4-286f3b5b06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_embedding(audio, sr):\n",
    "    audio = audio.mean(dim=0)\n",
    "    if sr != 16_000:\n",
    "        # Wav2Vec2 requires audio to be 16kHz\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16_000)\n",
    "        audio = resampler(audio)\n",
    "    audio = feature_extractor(\n",
    "        audio, sampling_rate=16_000, return_tensors=\"pt\"\n",
    "    ).input_values.cuda()\n",
    "    with torch.no_grad():\n",
    "        audio_embeddings = audio_model(audio).last_hidden_state.cpu().squeeze()\n",
    "    return audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0734bb-d38d-4f31-a583-7bca5a8b8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "\n",
    "# similarly for text, we are using XLM-RoBERTa which was trained on multilingual data\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\", cache_dir=\"models/\")\n",
    "text_model = XLMRobertaModel.from_pretrained(\n",
    "    \"xlm-roberta-base\", cache_dir=\"models/\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5952852-48f6-40dc-887f-0c0f5154823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_embedding(text: str):\n",
    "    encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoded_input = {key: val.cuda() for key, val in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_model(**encoded_input).last_hidden_state.cpu().squeeze()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "633844d7-a11f-44a2-a420-1a80182af03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 256\n"
     ]
    }
   ],
   "source": [
    "from batch_face import RetinaFace\n",
    "from models_multiview import FrontaliseModelMasks_wider\n",
    "\n",
    "# for visual embeddings, we are first using RetinaFace to find a cropping box for the face\n",
    "# and then we are using FAb-Net to extract visual facial embeddings\n",
    "detector = RetinaFace(0, model_path=\"./models/RetinaFace/mobilenet0.25_Final.pth\")\n",
    "visual_model = FrontaliseModelMasks_wider(\n",
    "    input_nc=3, inner_nc=256, num_additional_ids=32\n",
    ")\n",
    "visual_model.load_state_dict(\n",
    "    torch.load(\"./models/fabnet/affectnet_4views.pth\")[\"state_dict_model\"]\n",
    ")\n",
    "visual_model = visual_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fd9897fe-6cd0-44de-bf59-f678b36148fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_face(faces: list):\n",
    "    # a lot of videos in CMU-MOSEI come from DVD reviews. These often have actors faces\n",
    "    # in the frame. A good heuristic for finding the face of the speaker is finding the\n",
    "    # largest face in the frame.\n",
    "    ix = -1\n",
    "    max_area = -1\n",
    "    for i, face in enumerate(faces):\n",
    "        bbox, _, conf = face\n",
    "        if conf < 0.99:\n",
    "            continue\n",
    "        face_width = bbox[2] - bbox[0]\n",
    "        face_height = bbox[3] - bbox[1]\n",
    "        face_area = face_width * face_height\n",
    "        if face_area > max_area:\n",
    "            ix = i\n",
    "            max_area = face_area\n",
    "\n",
    "    if ix != -1:\n",
    "        return faces[ix]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e637553-02ec-48d7-a417-bac1673bd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FACE_WIDTH = 30\n",
    "MIN_FACE_HEIGHT = 60\n",
    "MIN_FACE_CONFIDENCE = 0.95\n",
    "\n",
    "\n",
    "def check_faces(faces: list):\n",
    "    # some videos do not have a face in the frame. we reject these\n",
    "    # as they won't be useful as training data.\n",
    "    if len(faces) != 1:\n",
    "        face = find_largest_face(faces)\n",
    "        if face is None:\n",
    "            return None\n",
    "    else:\n",
    "        face = faces[0]\n",
    "    bbox, _, conf = face\n",
    "    if conf < MIN_FACE_CONFIDENCE:\n",
    "        return None\n",
    "    face_width = bbox[2] - bbox[0]\n",
    "    face_height = bbox[3] - bbox[1]\n",
    "    if face_width < MIN_FACE_WIDTH:\n",
    "        return None\n",
    "    elif face_height < MIN_FACE_HEIGHT:\n",
    "        return None\n",
    "    return bbox.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "646f862f-6c45-4f5f-8276-200ba19c707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(frame_face):\n",
    "    # we crop the face to a 256x256 bounding box,\n",
    "    # adding padding if necessary to center the face.\n",
    "\n",
    "    frame, face = frame_face\n",
    "    # frame = (height, width, channels)\n",
    "    # face = (x_start, y_start, x_end, y_end)\n",
    "    x_start, y_start, x_end, y_end = face\n",
    "    x_center = (x_start + x_end) / 2\n",
    "    y_center = (y_start + y_end) / 2\n",
    "\n",
    "    width = x_end - x_start\n",
    "    height = y_end - y_start\n",
    "    if width > height:\n",
    "        length = width\n",
    "    else:\n",
    "        length = height\n",
    "\n",
    "    pad = int(length * 0.2)\n",
    "\n",
    "    padded_x_start = int(x_center - length / 2 - pad)\n",
    "\n",
    "    padded_y_start = int(y_center - length / 2 - pad)\n",
    "\n",
    "    frame = torch.tensor(frame).permute(2, 0, 1)\n",
    "    face_frame = F_vision.resized_crop(\n",
    "        frame,\n",
    "        top=padded_y_start,\n",
    "        left=padded_x_start,\n",
    "        height=length + 2 * pad,\n",
    "        width=length + 2 * pad,\n",
    "        size=[256, 256],\n",
    "        antialias=True,\n",
    "    )\n",
    "    return face_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7f7939cb-4055-46ac-a39c-8158a56cd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_visual_embedding(frames):\n",
    "    # the visual embedding pipeline consists of:\n",
    "    # 1. detect all faces in every frame\n",
    "    # 2. check the faces for speaker's face\n",
    "    # 3. crop the frame to center the face of speaker\n",
    "    # 4. extract embeddings via FAb-Net from the cropepd faces\n",
    "    faces_in_frames = detector.detect(frames, chunk_size=frames.shape[0])\n",
    "    faces_in_frames = map(check_faces, faces_in_frames)\n",
    "    face_frames = list(\n",
    "        map(\n",
    "            crop_face,\n",
    "            filter(\n",
    "                lambda frame_face: frame_face[1] is not None,\n",
    "                zip(frames, faces_in_frames),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    if len(face_frames) < 5:\n",
    "        print(\"Failed!\")\n",
    "        return None, None\n",
    "    face_frames = torch.stack(face_frames)\n",
    "    face_frames_cuda = F_vision.convert_image_dtype(face_frames).cuda()\n",
    "    with torch.no_grad():\n",
    "        visual_embeddings = visual_model.encoder(face_frames_cuda).cpu().squeeze()\n",
    "    return visual_embeddings, face_frames.permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "65d670c5-0654-4556-bcbe-2a1aa44afc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_face(face_frames: torch.Tensor, index: int):\n",
    "    plt.imshow(face_frames[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "74faf14e-8a43-4717-ad40-7613e25cf768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4355\n",
      "Metadata(path='data/CMU-MOSEI/Raw/298736/8.mp4',\n",
      "         video_id='298736',\n",
      "         clip_id=8,\n",
      "         duration=2.239,\n",
      "         width=480,\n",
      "         height=360,\n",
      "         sample_rate=32000,\n",
      "         channels=1,\n",
      "         text='(umm) I did not read the books',\n",
      "         sentiment_score=-0.6666666865348816,\n",
      "         sentiment=<Sentiment.NEGATIVE: 3>,\n",
      "         split=<Split.TRAIN: 1>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"data/CMU-MOSEI/Raw/298736/8.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(cmu_mosei))\n",
    "print(i)\n",
    "metadata = cmu_mosei[i]\n",
    "metadata.play_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b8ddd141-ebc5-41d4-aa0d-e332e5b31c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embedding = extract_audio_embedding(*metadata.get_audio())\n",
    "text_embedding = extract_text_embedding(metadata.text)\n",
    "visual_embedding, face_frames = extract_visual_embedding(\n",
    "    metadata.get_video_frames(fps=30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f4640659-29c3-4564-a8e9-8cb3e6e5607d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cb7bd5b7-1131-47f8-9204-cef4319bd5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([111, 1024])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7c3e93ab-9b6d-4d59-85cf-760aeb16ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 256])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b3082a1f-7687-4ec0-a6e4-50936fa7f5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbaf612651e46afb19c5ad3ecafc2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=27, description='index', max=54), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = widgets.interact(\n",
    "    plot_face,\n",
    "    index=(0, len(face_frames) - 1, 1),\n",
    "    face_frames=widgets.fixed(face_frames),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13d5894e-5d24-422f-affc-26bd53e9302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Datum:\n",
    "    metadata: Metadata\n",
    "    audio_embedding: torch.FloatTensor\n",
    "    visual_embedding: torch.FloatTensor\n",
    "    text_embedding: torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34d74585-a441-47dd-8e2e-8979001c7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                      | 39/4403 [02:41<5:01:57,  4.15s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ch_sims_dataset = []\n",
    "failed = []\n",
    "for i, metadata in enumerate(tqdm(ch_sims)):\n",
    "    if metadata.duration > 10:\n",
    "        continue\n",
    "    visual_embedding = extract_visual_embedding(metadata.get_video_frames(fps=15))\n",
    "    if visual_embedding is None:\n",
    "        failed.append(i)\n",
    "        continue\n",
    "    audio_embedding = extract_audio_embedding(*metadata.get_audio())\n",
    "    text_embedding = extract_text_embedding(metadata.text)\n",
    "    ch_sims_dataset.append(\n",
    "        Datum(\n",
    "            metadata=metadata,\n",
    "            audio_embedding=audio_embedding,\n",
    "            visual_embedding=visual_embedding,\n",
    "            text_embedding=text_embedding,\n",
    "        )\n",
    "    )\n",
    "print(f\"{len(ch_sims_dataset) = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
